# 9장. 웹 크롤러 설계

---

웹 크롤러는 Robot, Spider라고도 부른다.

검색 엔진에서 주로 쓰이는 기술

**웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주 목적** 

몇 개의 웹 페이지에서 시작하여 링크를 따라가면서 새로운 콘텐츠들을 수집하는 방식으로 동작한다.

![image](https://github.com/ZI-won-ZONE-ha/CS_JONGJIBU/assets/88527476/773d743f-873a-45bf-86bb-03ad6b1177a3)

크롤러의 사용 사례

- 검색 엔진 인덱싱: 가장 보편적인 사용 방식으로 **웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스를 생성**
- 웹 아카이빙: 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차
- 웹 마이닝: 웹 마이닝을 통해 인터넷에서 유용한 지식을 도출해 낼 수 있다.
- 웹 모니터링: 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링할 수 있다.

## 문제 이해 및 설계 범위 확정

---

웹 크롤러의 기본 알고리즘

1. URL 집합이 입력으로 주어지면 해당 URL들이 가리키는 모든 웹 페이지를 다운로드한다.
2. 다운받은 웹 페이지에서 URL들을 추출한다.
3. 추출된 URL들은 다운로드할 URL 목록에 추가하고 위 과정을 처음부터 반복한다.

기본 알고리즘은 간단하지만 실제로 이렇게 단순하게 동작하지는 않는다.

→ 규모 확장성을 고려하여 설계해야 됨

알아내야 할 요구사항들

- 크롤러의 주된 용도: 검색 엔진, 마이닝 등
- 매달 수집하는 데이터의 수
- 새로 만들어진 웹 페이지, 수정된 웹 페이지 고려 여부
- 수집한 웹 페이지 저장 여부 (기간 등)
- 중복된 콘텐츠를 가지는 페이지 처리 방법

좋은 웹 크롤러가 만족시켜야 할 속성들

- 규모 확장성: 병행성을 활용하면 효과적으로 거대한 웹을 크롤링 할 수 있다.
- 안정성: 웹은 함정이 많다. → 비정상적인 입력이나 환경에 대응해야 됨
- 예절: 크롤러는 수집 대상 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내면 안 된다.
- 확장성: 새로운 형태의 콘텐츠를 지원하기 쉬워야 한다. (이미지, 비디오 등 여러 콘텐츠가 추가될 때 새로 설계해야 되는 상황이 발생하면 안 됨)

### 개략적 규모 추정

- 매달 10억개의 웹 페이지 다운로드
- QPS = 10억 / 30일 / 24시간 / 3600초 = 400 페이지 / 초
- 최대 QPS = 2 * 400 = 800 QPS
- 웹 페이지 크기 평균은 500k로 가정
- 10억 페이지 * 500k = 500TB / 월
- 1개월 데이터 500TB → 5년 보관시 30PB의 저장 공간 필요

## 개략적 설계안 제시 및 동의 구하기

---

![image](https://github.com/ZI-won-ZONE-ha/CS_JONGJIBU/assets/88527476/d9038eb2-6b15-4aa3-a41a-6fae8f52ee96)

### 시작 URL 집합

웹 크롤러가 크롤링을 시작하는 출발점

예시) 어떤 대학 웹사이트로부터 찾아 나갈 수 있는 모든 웹 페이지를 크롤링하는 가장 직관적인 방법은 해당 대학의 도메인 이름이 붙은 모든 페이지의 URL을 시작점으로 사용

전체 웹을 크롤링하는 경우: 가능한 많은 링크를 탐색하기 위해 전체 URL 공간을 작은 부분 집합으로 나누는 전략 사용 → 지역적 특색, 주제 별로 다른 시작 URL을 사용하여 시작 URL 선택 가능

### 미수집 URL 저장소

웹 크롤러는 크롤링 상태를 두 가지로 나누어 관리

1. 다운로드할 URL
2. 다운로드된 URL

미수집 URL 저장소는 다운로드할 URL들을 모아두는 저장소 → 큐(FIFO) 구조를 가진다.

### HTML 다운로더

웹 페이지를 다운로드하는 컴포넌트, 미수집 URL 저장소가 저장할 URL 정보 제공

### 도메인 이름 변환기

웹 페이지를 다운 받으려면 URL을 IP 주소로 변환하는 절차가 필요함

→ **HTML 다운로더는 도메인 이름 변환기를 활용하여 URL에 대응되는 IP주소를 알아낸다.**

### 콘텐츠 파서

**웹 페이지를 다운로드하면 파싱과 검증 절차를 거쳐야 한다.**

→ 이상한 웹 페이지가 들어가게 하지 않기 위해서

### 중복된 컨텐츠인가?

**연구 결과에 따르면 웹의 29%는 중복**

→ 특정 컨텐츠가 중복되어 저장될 수 있다.

**자료구조를 도입하여 데이터 중복을 줄이고 처리에 소요되는 시간을 줄일 수 있다.**

→ 웹 페이지의 해시값을 비교하여 중복을 피할 수 있다.

### 컨텐츠 저장소

HTML 문서를 보관하는 시스템

**저장소는 저장할 데이터 유형, 크기, 저장소 접근 빈도 등 여러가지를 고려해야 한다.**

디스크와 메모리 동시에 사용하여 구현 가능

- 데이터 양이 많으므로 디스크에 데이터를 저장해야 한다.
- 자주 찾는 데이터는 메모리(캐시)에 두어 접근 지연을 줄일 수 있다.

### URL 추출기

HTML을 파싱하여 링크들을 골라내는 역할을 수행

상대 경로는 전부 HTML 페이지의 주소를 붙여 절대 경로로 변환

### URL 필터

특정 콘텐츠 타입, 파일 확장자를 가지는 URL, 접속이 안되는 URL, 접근 제외 목록 URL 등을 필터링하는 용도

### 이미 방문한 URL?

이미 방문한 URL이나 미수집 URL 보관소에 보관된 URL을 추적할 수 있도록 자료 구조 활용 가능

**이미 방문한 URL을 처리할 수 있다면 서버 부하 감소 및 무한 루프 방지가 가능하다.**

**Bloom Filter나 Hash Table이 주로 사용된다.**

### URL 저장소

이미 방문한 URL을 보관하는 저장소

### 웹 크롤러 작업 흐름

1. 시작 URL들을 미수집 URL 저장소에 저장
2. HTML 다운로더는 미수집 URL 저장소에서 URL을 가져온다.
3. 도메인 이름 변환기를 활용하여 URL → IP로 매핑하고 IP 주소에 접근하여 웹 페이지 다운로드
4. 콘텐츠 파서가 다운로드 된 HTML 페이지를 파싱하여 올바른 페이지인지 검증한다.
5. 중복 컨텐츠 확인 작업을 가진다.
6. 중복 확인을 위해 해당 페이지가 이미 저장소에 있는지 확인한다.
    1. 이미 저장소에 있다면 버린다.
    2. 저장소에 없다면 저장소에 저장하고 URL 추출기로 전달한다.
7. URL 추출기는 해당 HTML 페이지에서 링크를 골라낸다.
8. 골라낸 링크를 URL 필터로 전달
9. 필터링 된 URL들의 중복 검사를 진행한다.
10. 이미 처리한 URL인지 확인하기 위해 URL 저장소에 보관된 URL인지 확인 → 이미 처리 되었다면 버린다.
11. 저장소에 없는 URL이면 URL 저장소에 저장하면서 미수집 URL 저장소에도 전달

## 상세 설계

---

### DFS? BFS?

웹은 유향 그래프이다.(방향 존재)

**DFS는 그래프 크기가 큰 경우 어느 depth까지 들어갈 지 예측할 수 없어 추천되지 않음**

→ **보통 BFS를 통한 탐색을 진행하게 된다!**

BFS 구현법의 문제

- 한 페이지에서 나오는 링크의 상당수는 같은 서버로 되돌아간다. → **병렬적으로 처리하게 되면 하나의 호스트에 여러 요청을 단기간에 보내게 되어 예의없는 크롤러로 간주될 수 있다.**
- BFS 알고리즘은 URL 간의 우선순위를 두지 않음 → 처리 순서가 공평하다 → **웹 페이지는 모두가 같은 수준의 중요성을 가지지 않으므로 우선순위가 필요한 문제가 있음**

### 미수집 URL 저장소

위의 BFS 구현법의 문제를 미수집 URL 저장소를 통해 해결할 수 있다.

→ **예의를 갖추면서 우선순위와 신선도를 관리할 수 있는 웹 크롤러로 설계할 수 있다.**

미수집 URL 저장소: 다운로드 할 URL을 보관하는 장소이다.

**예의**

**웹 크롤러는 수집 대상 서버에 너무 많은 요청을 단기간 보내면 안된다.**

→ DoS 공격으로 오해할 수 있음

**예의바른 크롤러의 원칙: 동일 웹 사이트에는 한 번에 한 페이지만 요청하기**

→ 시간차를 두고 실행하도록 설계해야 된다.

이런 요구사항을 만족하기 위해 **호스트명과 작업 스레드 사이의 관계를 유지시키야 한다.**

→ 다운로드 스레드는 별도의 FIFO 큐를 가지고 해당 큐에서 꺼낸 URL만 다운로드 한다.

![image](https://github.com/ZI-won-ZONE-ha/CS_JONGJIBU/assets/88527476/017256e6-a1eb-41ef-abdb-eb5906c0306d)

큐 라우터: 같은 호스트에 속한 URL은 같은 큐로 가도록 보장하는 역할을 한다.

매핑 테이블: 호스트 이름과 큐 사이의 관계를 보관하는 테이블

큐: 같은 호스트에 속한 URL은 같은 큐에 보관한다.

큐 선택기: 큐들을 순회하면서 큐에서 URL을 꺼내서 나온 URL을 다운로드하도록 작업 스레드에 전달

작업 스레드: 전달된 URL을 다운로드하는 작업을 수행한다. 순차적으로 URL들이 처리된다. 작업 간 딜레이를 줄 수 있음

**우선순위**

URL 사이에서 더 중요한 것이 있을 수 있다.

→ 우선순위 설정이 필요하다.

**유용성을 기준으로 나눈다면 페이지 랭크, 트래픽, 갱신 빈도 등 다양한 척도 활용 가능**

**순위 결정 장치를 통해 URL에 우선순위를 줄 수 있다.**

![image](https://github.com/ZI-won-ZONE-ha/CS_JONGJIBU/assets/88527476/af9a3815-502f-4065-a488-8dc99332545b)

순위결정장치: URL의 우선순위를 계산한다.

큐: 우선순위별로 큐가 하나씩 할당된다. → 우선순위가 높은 큐가 더 자주 할당 됨

큐 선택기: 순위가 높은 큐를 더 자주 선택한다.

위 2가지 예의와 우선순위를 적용한 최종적인 설계는 다음과 같다.

![image](https://github.com/ZI-won-ZONE-ha/CS_JONGJIBU/assets/88527476/e561a81f-bea5-48d0-90ac-796283b35db9)

**신선도**

웹 페이지는 수시로 추가되고, 삭제되고, 변경된다. **데이터의 신선함을 유지하기 위해서는 이미 다운로드한 페이지라도 주기적으로 재수집 해야 된다.**

재수집 전략

- 웹 페이지의 변경 이력 활용
- 우선순위를 활용하여 중요한 페이지는 더 자주 재수집

**미수집 URL 저장소를 위한 지속성 저장장치**

모든 데이터를 메모리에 저장하는 것은 안정성, 규모 확장성 면에서 좋지 않음

전부 디스크에 저장하는 것은 느린 속도 때문에 병목 지점이 될 수 있음

→ 대부분의 URL은 디스크에 두고 IO 비용 감소를 위해 메모리 버퍼에 큐를 두는 방법으로 두 방식을 섞어서 활용

### HTML 다운로더

**Robots.txt (로봇 제외 프로토콜)**

웹 사이트가 크롤러와 소통하는 표준적인 방법

→ 크롤러가 수집해도 되는 페이지 목록이 들어있다.

→ 크롤러가 웹 사이트를 긁어가기 전에 해당 파일에 나열된 규칙을 먼저 확인해야 된다.

**성능 최적화**

HTML 다운로더에 사용할 수 있는 성능 최적화 기법들

1. 분산 크롤링

크롤링 작업을 여러 서버에 분산하는 방법

각 서버는 여러 스레드를 활용하여 다운로드 작업을 처리

**URL 공간을 작은 단위로 분할하여 각 서버가 한 단위를 다운로드하도록 설계**

1. 도메인 이름 변환 결과 캐시

크롤러 성능 병목 중 하나 → **DNS 요청을 보내고 결과를 받는 작업의 동기적 특징으로 인해 병목 발생**

→ DNS 조회 결과로 얻어진 도메인 이름과 IP 주소 사이의 관계를 캐싱해두면 해당 요청을 빠르게 진행하여 병목을 줄일 수 있음

1. 지역성

크롤링 서버를 지역별로 분산하는 방법

→ 크롤링 대상 서버와 크롤링 서버가 가까우면 물리적으로 다운로드 시간이 줄어들 수 있음

1. 짧은 타임아웃

어떤 웹 서버는 응답이 느리거나 응답하지 않는다. 이런 경우를 대비하여 요청 타임아웃을 걸어두는게 좋다.

**안정성**

안정성도 다운로더 설계 시 중요하게 고려할 부분이다.

- 안정 해시: 다운로더 서버들에 부하를 분산할 때 적용 가능한 기술 (5장 참고)
- 크롤링 상태 및 수집 데이터 저장: 장애가 발생했을 때 쉽게 복구할 수 있도록 크롤링 상태와 수집된 데이터를 지속적 저장장치에 기록해 두는 것이 바람직
- 예외 처리: 예외가 발생해도 전체 시스템이 중단되지 않도록 설계해야 된다.
- 데이터 검증: 시스템 오류를 방지하기 위한 중요 수단 중 하나

**확장성**

시스템은 계속 변하므로 확장을 고려하여 설계해야 한다.

→ **새로운 형태의 콘텐츠를 쉽게 지원할 수 있도록 설계해야 됨**

**문제 있는 콘텐츠 감지 및 회피**

1. 중복 컨텐츠

해시나 체크섬을 활용하면 중복 컨텐츠 감지를 쉽게 할 수 있다.

1. 거미 덫 (Spider Trap)

**크롤러를 무한 루프에 빠뜨리도록 설계한 웹 페이지**

→ URL의 최대 길이를 제한하면 회피할 수 있다.

다만 모든 덫을 막을 수 없다. → 수작업으로 덫을 찾고 크롤러 탐색에 제외하는 방법도 하나의 방법이 될 수 있음

1. 데이터 노이즈

데이터 자체가 아무런 가치가 없으면 크롤링 할 이유가 전혀 없다. → 가능하면 제외

## 마무리

---

추가적으로 논의할 사항

- 서버 측 렌더링: 동적으로 생성되는 링크를 확인하기 위해 필요한 방법
- 원치 않는 페이지 필터링: 자원을 낭비하지 않도록 스팸 방지 컴포넌트를 두어 쓸모없는 데이터는 필터링 해야 된다.
- 데이터베이스 다중화 및 샤딩: 데이터 계층의 가용성, 규모 확장성, 안정성을 향상시킬 수 있음
- 수평적 규모 확장성: 수 많은 데이터를 크롤링하려면 많은 서버가 필요함 → 무상태 계층으로 서버를 설계해야 수평적 규모 확장 가능
- 가용성, 일관성, 안정성
- 데이터 분석 솔루션
